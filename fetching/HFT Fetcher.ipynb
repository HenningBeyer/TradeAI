{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353d455e",
   "metadata": {},
   "source": [
    "------\n",
    "# High-Frequency Data Maintainment\n",
    "    - Fetch new HFT data occasionally\n",
    "    - Include new currencies occasionally\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b8a3a",
   "metadata": {},
   "source": [
    "---\n",
    "#### General Notes on HFT Data Fetching:\n",
    "\n",
    "    - Remember to create a new dir for dataset fetching period!\n",
    "    - Live millisecond data will never be available again in a wished frequency\n",
    "    - So remenber to fetch in advance!\n",
    "    - All currencies are fetched in parallel via multiprocessing\n",
    "    - Time of data is UTC (-1h from Europe)\n",
    "    \n",
    "#### Notes on Data\n",
    "    - 5 most relevant bid-ask-prices and volumes are fetched with a response delay (21 base features + time index) \n",
    "    - The Time index is when a fetching request was issued and received with a delay\n",
    "    - This is all data adviced inside HFT. \n",
    "    - External features will generally be useless inside HFT.\n",
    "    - Feature engineering can extract a lot of information these base features\n",
    "    - Binance will provide Close prices in only another request. Thus they are not fetched.\n",
    "    \n",
    "#### Notes on Request Delay\n",
    "\n",
    "    - A fetch request is typically answered after ~250 ms and for ~900 on occasional checkups\n",
    "    - The 'Time' index is the time when the data is received\n",
    "    - The 'Response Delay' is the time after an issued request is fullfilled with data\n",
    "        • Includes the time of sending a request to Binance (about 200 ms of 'Response Delay')\n",
    "        • Includes the time of Binance responding with data (about  50 ms of 'Response Delay')\n",
    "        • So the real time delay of data a model faces is ~50 ms and the fetching time offset is ~200 ms\n",
    "        • There is no found way to capture these separately with Binance or ccxt\n",
    "\n",
    "          \n",
    "    - Coins are chosen for HFT if (MarketCap > 500M and 24hVolume > 50M) for https://www.binance.com/en/markets/overview\n",
    "    - These are mostly the coins also viable for scalping (all coins from scalping are included at the time)\n",
    "    - Check all Binance live orderbook data in: https://www.binance.com/en/markets/overview   \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d71a88",
   "metadata": {},
   "source": [
    "---\n",
    "### Further Practical Notes on HFT Data Fetching:\n",
    "\n",
    "    Delays:\n",
    "    - Request delays (when sending + validating) range from 200-230 ms in Europe and 20-30 ms on Tokyo AWS servers.\n",
    "    - Response delays (when Binance sends) only last less than 15 ms in Europe and less in Tokyo.\n",
    "    - Longer response delays of 0.9 s in Europe and ?.? s in Tokyo can occur when Binance does an initial checkup of the IP\n",
    "    \n",
    "    Used Method:\n",
    "    - The first version of fetching involved tacted HTTP requests via ccxt.binance\n",
    "        + tacted fetches in 100 ms steps possible (19:10.100 --> 19:10.200 --> 19:10.300, ...) (from Tokyo)\n",
    "        \n",
    "        - random request delays on Binance's IP checkups (~900 ms) both on Europe and Tokyo\n",
    "        - The request limit is a merely 20 requests per second\n",
    "        - Thos makes fetching multiple currencies concurrently impossible\n",
    "        - delays are long from europe (250 ms) and only good on AWS Tokyo\n",
    "     \n",
    "     - The now most recent version uses 'websocket' from Binance:\n",
    "        + No problem with requesting limits\n",
    "        + 100 ms tacted data minimally (lowest reponse freq from Binance) (will be enough)\n",
    "        + only 1 ms response offset and rarely just 15 ms\n",
    "        + No occasional IP checkups with 900 ms delays\n",
    "        + less nan values due to delays, better data for RL and ML\n",
    "        + simplicity\n",
    "        \n",
    "        - An incremental response delay (~15 ms) hinders the tacting in steps like (19:10.100 --> 19:10.200, ...)\n",
    "\n",
    "        --> new version gets better data more easily\n",
    "    \n",
    "    Disk Space and RAM\n",
    "    - Fetching locally from a Jupyter notebook forces one to keep the local machine running\n",
    "    - There will be 864.000 data points (~190 MB) per day for each currency on 100 ms frequencies\n",
    "    - One day of 100 ms data will eat up 300 MB of RAM for the program with 20 base features\n",
    "    \n",
    "        • Total disk space = 50 pairs * 30 d * 190 MB \n",
    "                           = ~ 285 GB per month for 50 currencies\n",
    "                           = ~   6 GB per month for one currency\n",
    "                           • Storing 250 GB on AWS Glacier: 1 € per month\n",
    "                           \n",
    "        • Total RAM for fetching ~ less than 2 GB\n",
    "    \n",
    "    Goal of Data\n",
    "    - It is adviced to fetch an extensive train dataset in 100 ms freq\n",
    "        • It will be viable for upsampling in 250 and 500 ms \n",
    "        • A lot of data will be useful for a robust RL pre-training \n",
    "          which includes different starting and ending times of episodes!\n",
    "        • Multiple extensive train datasets with diverse time series\n",
    "          characteristics will improve a RL model significantly\n",
    "    - Moreso, it is adviced to keep fetching over multiple months without breaks\n",
    "        • It ensures always the newest data\n",
    "        • It wont miss out on any interesting events\n",
    "        • Flexibly having data for customers or experiments will prove very handy\n",
    "    - It is also adviced to fetch datasets for every currency synchronically to other datasets\n",
    "      for comparibility\n",
    "        \n",
    "        \n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350713f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38716251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.error import  HTTPError\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import ccxt\n",
    "from ccxt.base.errors import RateLimitExceeded, BadSymbol\n",
    "import tqdm\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import threading\n",
    "import json\n",
    "import websocket\n",
    "\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1815ac1",
   "metadata": {},
   "source": [
    "### Datastream Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2ed953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/binance/binance-spot-api-docs/blob/master/web-socket-streams.md\n",
    "class HFT_Datastream:\n",
    "    \"\"\" Fetches one pair in a regular 100 ms frequency\n",
    "    \n",
    "        One fetcher for one pair is mandatory, as responsemessages wont be distinguishable.\n",
    "        \n",
    "        -\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\" A class that fetches the 5 most recent orderbook entries a single Binance \n",
    "        pair in tacted 100 millisecond intervals. It uses an independent thread for that.\n",
    "        \n",
    "        \n",
    "        Using websockets from binance, these request delays only occur for sending requests\n",
    "        It provides a total of 21 base features including the top 5 bid prices, bid volumes, \n",
    "        ask prices, ask volumes and 1 request delay\n",
    "    \n",
    "        Functionality:\n",
    "            - These mentioned above\n",
    "            - Keeps exavtly one np.array() as the most recent fetch that can get accessed by a recorder class\n",
    "            - Stores info if a new fetch is available after the last fetch via 'fetcher.fetch_is_updated'\n",
    "                - The fetcher may skips entire fetch intervals while keeping 'fetcher.fetch_is_updated' == 'False'\n",
    "            - Will conveniently start at times of [12:15:01.000, 12:15:01.150, 12:15:01.300, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.coin           = params['coin'          ] # 'BTC'\n",
    "        self.stable_coin    = params['stable_coin'   ] # 'USDT'\n",
    "        self.stream_freq    = params['stream_freq'   ] # in ms; multiple of 100 ms\n",
    "        self.notified_insts = params['notified_insts'] # list of python objects\n",
    "        self.num_of_entries = params['num_of_entries'] # only 5, 10 and 20 possible\n",
    "        \n",
    "        assert (self.stream_freq == 100)\n",
    "        socket='wss://stream.binance.com:9443/ws'\n",
    "        self.ws = websocket.WebSocketApp(socket, on_open    = self._start_subscription, \n",
    "                                                 on_message = self._on_message)\n",
    "        \n",
    "        self.message_num       = 1\n",
    "        self.thread_is_running = False\n",
    "        self.fetch_is_updated  = {str(inst) : False for inst in self.notified_insts}\n",
    "        self.last_live_fetch   = None\n",
    "        self.last_receive_time = None\n",
    "        self.thread            = threading.Thread(target = self.ws.run_forever, args = ())   \n",
    "        self.ticker            = self.coin + self.stable_coin\n",
    "        \n",
    "    def __del__(self):\n",
    "        \"\"\" deleting HFT_Datastream properly.\n",
    "            \n",
    "            'del recorder' wont work. \n",
    "            Jupyter does call __del__() on overwriting class variables.\n",
    "        \"\"\"\n",
    "        self.thread_is_running = True \n",
    "        self.stop_datastream()\n",
    "\n",
    "    def start_datastream(self):\n",
    "        assert self.thread_is_running == False\n",
    "        self.thread_is_running = True\n",
    "        self.thread.start()\n",
    "            # self.thread.start() calls self.ws.run_forever()\n",
    "            # self.ws.run_forever() calls _start_subscription()\n",
    "            # and then, _on_message() will be invoked for every subscription message()\n",
    "\n",
    "    def stop_datastream(self):\n",
    "        assert self.thread_is_running == True\n",
    "        self.ws.close()\n",
    "        self.thread.join()\n",
    "        \n",
    "    def get_fetch_status(self, inst):\n",
    "        return self.fetch_is_updated[str(inst)]\n",
    "        \n",
    "    def get_fetch(self, inst): # inst is the object that accesses the data with get_data\n",
    "        self.fetch_is_updated[str(inst)] = False\n",
    "        return self.last_live_fetch\n",
    "\n",
    "    def _start_subscription(self, ws):\n",
    "        for inst in self.notified_insts: \n",
    "            inst.received_data = False\n",
    "        sub_message = {\n",
    "            \"method\": \"SUBSCRIBE\",\n",
    "            \"params\": # like ['btcusdt@depth5@100ms', 'adausdt@depth5@100ms', ...]\n",
    "                [f\"{self.ticker.lower()}@depth{self.num_of_entries}@{self.stream_freq}ms\"],\n",
    "             \"id\": 1\n",
    "             }\n",
    "        \n",
    "        # UTC time; start at the next full second\n",
    "        planned_req_time = pd.to_datetime(np.ceil(time.time())*10**9) \n",
    "        \n",
    "        # Wait until the next timestep start\n",
    "        first_waiting_time = planned_req_time.value/(10**9) - time.time()\n",
    "        if first_waiting_time < 0:\n",
    "                first_waiting_time = 0.0\n",
    "        time.sleep(first_waiting_time)\n",
    "        \n",
    "        self.ws.send(json.dumps(sub_message))\n",
    "        \n",
    "        print(f\"Opened websocket subscription for {self.tickers}\") \n",
    "\n",
    "    def _on_message(self, ws, message): # started after subscription on every data message\n",
    "        msg = json.loads(message)\n",
    "        if 'lastUpdateId' in msg:  # filtering subsctription responses\n",
    "            if (self.message_num*100 % self.stream_freq == 0): # fetching every 2nd data on self.stream_freq == 200ms\n",
    "                self.message_num = 1\n",
    "                response_time          = pd.to_datetime(time.time()*10**9)\n",
    "                data_arr               = np.array([response_time]) \n",
    "                self.last_live_fetch   = np.hstack((data_arr, np.concatenate(\n",
    "                                                           (msg['bids'][0:5], msg['asks'][0:5]), \n",
    "                                                            axis=None) ))\n",
    "                self.fetch_is_updated  = {str(inst) : True for inst in self.notified_insts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13290d89",
   "metadata": {},
   "source": [
    "### Recorder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9975d174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class HFT_Sample_Recorder():\n",
    "    \"\"\" A class for storing and updating a list of fetches for other classes.\n",
    "        It can operate multiple HFT_Sample_Fetcher instances for each currencies\n",
    "    \n",
    "        Functionality:\n",
    "            - reads the data of HFT_Datastream instances and records their data\n",
    "            - Poses as a flexible storage of all pairs for multiple classes\n",
    "            - Data can be cleared or accessed by multiple other classes\n",
    "            - It will provide new data only once it is available; the rarely skipped time steps will not get stored\n",
    "            \n",
    "        Note:\n",
    "            - Change this class when using it for live fetching inside a RL environment\n",
    "              --> reduces delay by 15 ms when there is no separate class that accesses a recorder\n",
    "              --> a recorder can then be build on top of the rl live fetcher to avoid further delays\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 params):\n",
    "        \n",
    "        self.coins              = params['coins'         ] # like ['BTC', 'ETH', ...]\n",
    "        self.stable_coin        = params['stable_coin'   ] # 'USTD'\n",
    "        self.stream_freq        = params['stream_freq'   ] # like 0.25 (in s)   \n",
    "        self.num_of_entries     = params['num_of_entries'] # 5, 10 or 20\n",
    "        \n",
    "        self.pair_list     = [c + self.stable_coin for c in self.coins]\n",
    "        self.fetch_record  = {c + self.stable_coin : np.array([]) for c in self.coins}   \n",
    "        self.stream_insts = {c + self.stable_coin :     HFT_Datastream({'coin'           : c,\n",
    "                                                                        'stable_coin'    : self.stable_coin,\n",
    "                                                                        'stream_freq'    : self.stream_freq,\n",
    "                                                                        'num_of_entries' : self.num_of_entries,\n",
    "                                                                        'notified_insts' : [self]})\n",
    "                                 for c in self.coins}\n",
    "    \n",
    "        self.thread = threading.Thread(target = self.__recording_loop, args = ())\n",
    "        self.thread_is_running = False\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\" deleting HFT_Sample_Recorder properly.\n",
    "            Clears the recorder and fetcher threads.\n",
    "            \n",
    "            'del recorder' wont work. \n",
    "            Jupyter does call __del__() on overwriting class variables.\n",
    "        \"\"\"\n",
    "        self.thread_is_running = True \n",
    "        self.thread.join()\n",
    "        stream_insts   = [self.stream_insts[key] for key in self.stream_insts]\n",
    "        for stream_inst in stream_insts:\n",
    "            stream_inst.__del__()\n",
    "        \n",
    "    def clear_data(self):\n",
    "        self.fetch_record = {k : np.array([]) for k in self.fetch_record}\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.fetch_record\n",
    "    \n",
    "    def get_currency_data(self, pair):\n",
    "        return self.fetch_record[pair]\n",
    "         \n",
    "    def start_recording(self):\n",
    "        \"\"\" Opens a new thread for recording with __recording_loop() \"\"\"\n",
    "        assert self.thread_is_running == False\n",
    "        \n",
    "        self.thread_is_running = True\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop_recording(self):\n",
    "        \"\"\" Stops a current thread and the fetching of data \"\"\"\n",
    "        assert self.thread_is_running == True\n",
    "        self.thread_is_running = False # stops function __recording_loop; finishes the thread\n",
    "        self.thread.join() \n",
    "        stream_insts   = [self.stream_insts[key] for key in self.stream_insts]\n",
    "        for stream_inst in stream_insts:\n",
    "            stream_inst.stop_datastream()\n",
    "\n",
    "    def __start_synchronized_fetchers(self):\n",
    "        \"\"\" Function worked by the thread of the class \"\"\"  \n",
    "        \n",
    "        def _starting_func(stream_inst):\n",
    "            stream_inst.start_datastream()\n",
    "            \n",
    "        stream_insts   = [self.stream_insts[key] for key in self.stream_insts]  \n",
    "        pool = ThreadPool(processes=len(stream_insts))\n",
    "        pool.map_async(_starting_func, stream_insts)    \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "    def __recording_loop(self): \n",
    "        self.__start_synchronized_fetchers()\n",
    "        \n",
    "        while self.thread_is_running:\n",
    "\n",
    "            for pair in self.pair_list:\n",
    "                pair_fetch_is_updated = self.stream_insts[pair].get_fetch_status(self)\n",
    "                if pair_fetch_is_updated == True:\n",
    "                    _fetch  = self.stream_insts[pair].get_fetch(self)\n",
    "                    if len(self.fetch_record[pair]) != 0:\n",
    "                        self.fetch_record[pair] = np.vstack([self.fetch_record[pair], _fetch])\n",
    "                    else:                   \n",
    "                        self.fetch_record[pair] = _fetch\n",
    "            time.sleep(0.005) # NOTE: 15 ms of constant delay is caused by time.sleep(0.005) \n",
    "                              # local machine has minimum sleep time of 15 ms\n",
    "                              # minimum sleep time can be 1 ms on other machines on AWS\n",
    "            \n",
    "\n",
    "        print(f'Thread finished for HFT_Sample_Recorder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f84076",
   "metadata": {},
   "source": [
    "### Dataset Fetcher Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fc6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFT_Dataset_Fetcher():\n",
    "    \"\"\" A class for fetching multiple hft datasets using the class HFT_Sample_Recorder alongside multiprocessing. \n",
    "        \n",
    "        Extra functionality:\n",
    "            - Manages HFT_Sample_Recorder with abstraction\n",
    "            --> Will not interfer the data fetching while performing saving \n",
    "            --> Makes routinely backup saves and will then clear HFT_Sample_Recorder to avoid OOM errors\n",
    "            - HFT_Dataset_Fetcher fetches until manually stopped with inst.__del__()\n",
    "            - These backup saves or dataset fragments are then combined with a Dataset_Extractor()\n",
    "            - The backup saves are stored for later use and queried on demand with Dataset_Extractor()\n",
    "            --> Store all numerous backup saves on Glacier (50*5=250 GB per month)\n",
    "            --> Extract subsets to S3 Buckets (20*5*3d*150MB = ~100 GB on S3 bucket)   \n",
    "        File structure: (all created if not present)\n",
    "             hft data\n",
    "                - raw fetched data\n",
    "                    - since_2023-06-01 08:31\n",
    "                        - ADA_USDT_100ms_(2023-06-01-08꞉31)_(2023-06-01-09꞉30)_(1)\n",
    "                        - ADA_USDT_100ms_(2023-06-01-09꞉00)_(2023-06-01-09꞉30)_(2)\n",
    "                        - ...\n",
    "                    -  ...\n",
    "                - extracted datasets\n",
    "                    - from_(2023-06-01 12:00)_to_(2023-06-03 00:00)\n",
    "                        - ADA_USDT_100ms_(2023-06-01 12:00)_(2023-06-03 00:00)\n",
    "                        - BTC_USDT_100ms_(2023-06-01 12:00)_(2023-06-03 00:00)\n",
    "                        - ...\n",
    "                    - ...\n",
    "                \n",
    "        # '꞉' is unicode replacement (U+A789) of normal colon ':'\n",
    "        # Try to always save as .pkl files for halving disk space and feasible reading/writing times!\n",
    "        #   Use .csv saving for testing or demanded csv format only!\n",
    "        #   All .csv can be extracted from .pkl files for special demands\n",
    "                \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hft_dataset_fetcher_params):\n",
    "        \n",
    "        self.file_extension     = hft_dataset_fetcher_params['file_extension'  ] # 'pkl' or 'csv'\n",
    "        self.data_root_dir      = hft_dataset_fetcher_params['data_root_dir'   ] # 'hft data'\n",
    "        self.backup_interval    = hft_dataset_fetcher_params['backup_interval' ] # in min\n",
    "        self.coins              = hft_dataset_fetcher_params['coins'           ] # like ['BTC', 'ETH', ...]\n",
    "        self.stable_coin        = hft_dataset_fetcher_params['stable_coin'     ] # 'USTD'\n",
    "        self.stream_freq        = hft_dataset_fetcher_params['stream_freq'     ] # like 100 (in ms)\n",
    "\n",
    "        self.hft_recorder       = HFT_Sample_Recorder(hft_dataset_fetcher_params)\n",
    "        self.pair_list          = [c + self.stable_coin for c in self.coins]\n",
    "        \n",
    "        self.thread             = threading.Thread(target = self.__observing_loop, args = ())\n",
    "        self.thread_is_running  = False\n",
    "        \n",
    "        self._init_dirs()\n",
    "        \n",
    "        assert self.file_extension in ['pkl', 'csv']\n",
    "        assert self.backup_interval >= 1                   # At least one minute\n",
    "        assert type(self.backup_interval) == type(int(1))  # Only whole minutes\n",
    "\n",
    "        \n",
    "    def _init_dirs(self):\n",
    "        if os.path.exists(self.data_root_dir) == False:\n",
    "            os.mkdir(self.data_root_dir)\n",
    "            \n",
    "        self.backup_dir_path = self.data_root_dir + '/' + 'raw fetched data'\n",
    "        \n",
    "        \n",
    "        if os.path.exists(self.backup_dir_path ) == False:\n",
    "            os.mkdir(self.backup_dir_path)\n",
    "            \n",
    "        start_time           = str(pd.to_datetime(time.time()*10**9))[:16] # --> '2023-06-15 10:49'\n",
    "        bu_dir_name          = f'since_{start_time}'.replace(':', '꞉')\n",
    "        \n",
    "        self.backup_dir_path = self.backup_dir_path + '/' + bu_dir_name\n",
    "        \n",
    "        if os.path.exists(self.backup_dir_path ) == True:\n",
    "            for root, dirs, files in os.walk(self.backup_dir_path):\n",
    "                if len(files) != 0:\n",
    "                    raise Exception(f'Fetching directory {self.backup_dir_path} already in use. Wait a minute')\n",
    "        else:\n",
    "            os.mkdir(self.backup_dir_path)\n",
    "       \n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\" deleting HFT_Dataset_Fetcher properly.\n",
    "            Clears also recorder and fetcher threads.\n",
    "            \n",
    "            'del dataset_saver' wont work. \n",
    "            Jupyter does call __del__() on overwriting class variables.\n",
    "        \"\"\"\n",
    "        self.thread_is_running = True\n",
    "        self.stop_saving()\n",
    "        for root, dirs, files in os.walk(self.backup_dir_path):\n",
    "            if len(files) == 0:\n",
    "                os.rmdir(self.backup_dir_path)\n",
    "        self.hft_recorder.__del__()    \n",
    "     \n",
    "    \n",
    "    def start_saving(self):\n",
    "        \"\"\" Opens a new thread for recording with __observing_loop() \"\"\"\n",
    "        assert self.thread_is_running == False\n",
    "        self.thread_is_running = True\n",
    "        self.thread.start()\n",
    "      \n",
    "    \n",
    "    def stop_saving(self):\n",
    "        \"\"\" Stops a current thread and the fetching of data \"\"\"\n",
    "        assert self.thread_is_running == True\n",
    "        self.thread_is_running = False # stops function __recording_loop; finishes the thread\n",
    "        self.thread.join() \n",
    "        self.hft_recorder.stop_recording()\n",
    "        self.hft_recorder.clear_data()\n",
    "            \n",
    "            \n",
    "    def __observing_loop(self):  # backup saves, end_date, instances_to_fetch, backup_time!\n",
    "        \"\"\" Function worked by the thread of the class \"\"\"\n",
    "        \n",
    "        \"\"\" Does intervalled backup the data_recorder data and clears its data to save RAM\n",
    "        \n",
    "              (exactly slotted intervals: '10:49' --> '11:00' --> '11:30' --> '11:43')\n",
    "                                          (start)   (consecutive fetching)     (stop)  \n",
    "        \"\"\"  \n",
    "        self.hft_recorder.start_recording()\n",
    "        one_minute_time = 60*10**9\n",
    "        start_time      = time.time()\n",
    "        \n",
    "        self.backup_count  = 0\n",
    "        self.bu_start_date = str(pd.to_datetime(start_time*10**9))[:16] # --> '2023-06-01-10꞉49'\n",
    "        self.bu_end_date   = str(pd.to_datetime(start_time*10**9 + \\\n",
    "                                                self.backup_interval*one_minute_time + \\\n",
    "                                                int(int(self.bu_start_date[14:16]) % self.backup_interval)*one_minute_time)\n",
    "                                )[:16]                                  # --> '2023-06-01-11꞉00'\n",
    "        \n",
    "        print(f'Initialized dataset fetching for {len(self.coins)} pairs at {self.bu_start_date} UTC')\n",
    "        print(f'{self.backup_interval}-minutely backups expected starting at {self.bu_end_date} UTC')\n",
    "        \n",
    "        while self.thread_is_running:\n",
    "            \n",
    "            stop_time = pd.to_datetime(pd.to_datetime(self.bu_end_date).value) \n",
    "            backup_interval_end_reached = (time.time()*10**9 > stop_time.value)\n",
    "            if backup_interval_end_reached == True:\n",
    "                self.backup_count += 1\n",
    "                self.backup_save_all()\n",
    "                self.bu_start_date = self.bu_end_date\n",
    "                self.bu_end_date   = str(pd.to_datetime(pd.to_datetime(self.bu_start_date).value + \\\n",
    "                                                        one_minute_time*self.backup_interval))[:16]                    \n",
    "            time.sleep(0.010)\n",
    "\n",
    "        self.backup_count += 1\n",
    "        self.backup_save_all()\n",
    "\n",
    "        print(f'Thread finished for HFT_Dataset_Fetcher')\n",
    "        \n",
    "    \n",
    "    def backup_save_all(self): \n",
    "        \"\"\" Save a dataset for each ticker observed at every end of the backup interval. \"\"\"\n",
    "            \n",
    "        data = self.hft_recorder.get_data()\n",
    "        self.hft_recorder.clear_data() # free Memory\n",
    "        col_names  = ['Request Time',\n",
    "                      'Bid_1', 'BidV_1', 'Bid_2', 'BidV_2', 'Bid_3', 'BidV_3', 'Bid_4', 'BidV_4', 'Bid_5', 'BidV_5',\n",
    "                      'Ask_1', 'AskV_1', 'Ask_2', 'AskV_2', 'Ask_3', 'AskV_3', 'Ask_4', 'AskV_4', 'Ask_5', 'AskV_5']\n",
    "        for c in self.coins:\n",
    "            backup_fn      = f'{c}_{self.stable_coin}_{int(self.stream_freq)}ms_' + \\\n",
    "                             f'({self.bu_start_date})_({self.backup_count}).{self.file_extension}'\\\n",
    "                            .replace(':', '꞉')\n",
    "            backup_fn_path = self.backup_dir_path + '/' + backup_fn \n",
    "            data_key = c + self.stable_coin # 'BTCUSDT'\n",
    "            df = pd.DataFrame(data[data_key], columns = col_names) \n",
    "            df.set_index('Request Time', inplace=True)\n",
    "            if self.file_extension   == 'csv':\n",
    "                df.to_csv(   backup_fn_path) \n",
    "            elif self.file_extension == 'pkl':\n",
    "                df.to_pickle(backup_fn_path)\n",
    "            else:\n",
    "                raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf166646",
   "metadata": {},
   "source": [
    "### Initiate Continuous live fetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d9f90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset fetching for 44 pairs at 2023-06-18 15:04 UTC\n",
      "1-minutely backups expected starting at 2023-06-18 15:05 UTC\n",
      "time after coin ADA: 0.008999824523925781\n",
      "time after coin AGIX: 0.01992177963256836\n",
      "time after coin ALGO: 0.025922060012817383\n",
      "time after coin APT: 0.03592181205749512\n",
      "time after coin ARB: 0.04538130760192871\n",
      "time after coin ARPA: 0.05137920379638672\n",
      "time after coin ATOM: 0.06041121482849121\n",
      "time after coin AXS: 0.06541180610656738\n",
      "time after coin BNB: 0.07741332054138184\n",
      "time after coin BCH: 0.0844111442565918\n",
      "time after coin BTC: 0.09641122817993164\n",
      "time after coin CFX: 0.10241127014160156\n",
      "time after coin DASH: 0.10841178894042969\n",
      "time after coin DOGE: 0.16941165924072266\n",
      "time after coin DOT: 0.17841148376464844\n",
      "time after coin EOS: 0.18641233444213867\n",
      "time after coin EDU: 0.1964108943939209\n",
      "time after coin ETH: 0.2094106674194336\n",
      "time after coin ETC: 0.21641159057617188\n",
      "time after coin FET: 0.22341156005859375\n",
      "time after coin FIL: 0.23141098022460938\n",
      "time after coin FTM: 0.23841118812561035\n",
      "time after coin LDO: 0.24641108512878418\n",
      "time after coin NEO: 0.2534141540527344\n",
      "time after coin ONT: 0.26041126251220703\n",
      "time after coin OP: 0.26841044425964355\n",
      "time after coin PEPE: 0.2764110565185547\n",
      "time after coin KAVA: 0.28341078758239746\n",
      "time after coin LINK: 0.2914106845855713\n",
      "time after coin LTC: 0.2994108200073242\n",
      "time after coin MASK: 0.30541062355041504\n",
      "time after coin MATIC: 0.3685941696166992\n",
      "time after coin SAND: 0.37702393531799316\n",
      "time after coin SHIB: 0.38802361488342285\n",
      "time after coin SOL: 0.3980226516723633\n",
      "time after coin STX: 0.4090230464935303\n",
      "time after coin SUI: 0.4197103977203369\n",
      "time after coin TRX: 0.4267096519470215\n",
      "time after coin UNI: 0.4347095489501953\n",
      "time after coin VET: 0.4391012191772461\n",
      "time after coin WAVES: 0.4618649482727051\n",
      "time after coin XMR: 0.46886396408081055\n",
      "time after coin XRP: 0.4774501323699951\n",
      "time after coin ZEC: 0.4844503402709961\n",
      "time after coin ADA: 0.02099895477294922\n",
      "time after coin AGIX: 0.03499889373779297\n",
      "time after coin ALGO: 0.04199981689453125\n",
      "time after coin APT: 0.06200003623962402\n",
      "time after coin ARB: 0.07699966430664062\n",
      "time after coin ARPA: 0.08600163459777832\n",
      "time after coin ATOM: 0.10399961471557617\n",
      "time after coin AXS: 0.11100006103515625\n",
      "time after coin BNB: 0.12399935722351074\n",
      "time after coin BCH: 0.13199925422668457\n",
      "time after coin BTC: 0.15800094604492188\n",
      "time after coin CFX: 0.17100024223327637\n",
      "time after coin DASH: 0.17799973487854004\n",
      "time after coin DOGE: 0.19699954986572266\n",
      "time after coin DOT: 0.21300196647644043\n",
      "time after coin EOS: 0.22299981117248535\n",
      "time after coin EDU: 0.2409989833831787\n",
      "time after coin ETH: 0.26000022888183594\n",
      "time after coin ETC: 0.2689993381500244\n",
      "time after coin FET: 0.2779989242553711\n",
      "time after coin FIL: 0.2930002212524414\n",
      "time after coin FTM: 0.30699968338012695\n",
      "time after coin LDO: 0.31999969482421875\n",
      "time after coin NEO: 0.3299996852874756\n",
      "time after coin ONT: 0.3380002975463867\n",
      "time after coin OP: 0.3509993553161621\n",
      "time after coin PEPE: 0.36299872398376465\n",
      "time after coin KAVA: 0.36999940872192383\n",
      "time after coin LINK: 0.3880002498626709\n",
      "time after coin LTC: 0.40799999237060547\n",
      "time after coin MASK: 0.42800045013427734\n",
      "time after coin MATIC: 0.44499921798706055\n",
      "time after coin SAND: 0.45799875259399414\n",
      "time after coin SHIB: 0.4739987850189209\n",
      "time after coin SOL: 0.49199962615966797\n",
      "time after coin STX: 0.5089986324310303\n",
      "time after coin SUI: 0.5279989242553711\n",
      "time after coin TRX: 0.5459988117218018\n",
      "time after coin UNI: 0.5569987297058105\n",
      "time after coin VET: 0.5649991035461426\n",
      "time after coin WAVES: 0.5759997367858887\n",
      "time after coin XMR: 0.58500075340271\n",
      "time after coin XRP: 0.6079995632171631\n",
      "time after coin ZEC: 0.6169989109039307\n",
      "time after coin ADA: 0.013999462127685547\n",
      "time after coin AGIX: 0.02928638458251953\n",
      "time after coin ALGO: 0.03728604316711426\n",
      "time after coin APT: 0.05269169807434082\n",
      "time after coin ARB: 0.06834030151367188\n",
      "time after coin ARPA: 0.08101654052734375\n",
      "time after coin ATOM: 0.09301567077636719\n",
      "time after coin AXS: 0.10144901275634766\n",
      "time after coin BNB: 0.11981725692749023\n",
      "time after coin BCH: 0.12981796264648438\n",
      "time after coin BTC: 0.14908981323242188\n",
      "time after coin CFX: 0.16081619262695312\n",
      "time after coin DASH: 0.1698164939880371\n",
      "time after coin DOGE: 0.1875152587890625\n",
      "time after coin DOT: 0.19782543182373047\n",
      "time after coin EOS: 0.2078249454498291\n",
      "time after coin EDU: 0.22696733474731445\n",
      "time after coin ETH: 0.24629998207092285\n",
      "time after coin ETC: 0.2577223777770996\n",
      "time after coin FET: 0.2687239646911621\n",
      "time after coin FIL: 0.2877237796783447\n",
      "time after coin FTM: 0.29972195625305176\n",
      "time after coin LDO: 0.31472253799438477\n",
      "time after coin NEO: 0.3227217197418213\n",
      "time after coin ONT: 0.33272266387939453\n",
      "time after coin OP: 0.345721960067749\n",
      "time after coin PEPE: 0.3597221374511719\n",
      "time after coin KAVA: 0.36872339248657227\n",
      "time after coin LINK: 0.38172197341918945\n",
      "time after coin LTC: 0.3957219123840332\n",
      "time after coin MASK: 0.4087228775024414\n",
      "time after coin MATIC: 0.4257218837738037\n",
      "time after coin SAND: 0.43772196769714355\n",
      "time after coin SHIB: 0.4537231922149658\n",
      "time after coin SOL: 0.4697225093841553\n",
      "time after coin STX: 0.483722448348999\n",
      "time after coin SUI: 0.5007226467132568\n",
      "time after coin TRX: 0.5107231140136719\n",
      "time after coin UNI: 0.5197257995605469\n",
      "time after coin VET: 0.5327227115631104\n",
      "time after coin WAVES: 0.5437226295471191\n",
      "time after coin XMR: 0.554722785949707\n",
      "time after coin XRP: 0.572723388671875\n",
      "time after coin ZEC: 0.5847232341766357\n"
     ]
    }
   ],
   "source": [
    " coins = ['ADA','AGIX','ALGO','APT','ARB','ARPA','ATOM','AXS','BNB','BCH','BTC','CFX','DASH','DOGE','DOT','EOS','EDU',\n",
    "          'ETH','ETC','FET','FIL','FTM','LDO','NEO','ONT', 'OP', 'PEPE','KAVA','LINK','LTC','MASK','MATIC','SAND','SHIB',\n",
    "          'SOL','STX','SUI','TRX','UNI','VET','WAVES','XMR','XRP','ZEC'] \n",
    "\n",
    "params = {'file_extension' : 'csv',\n",
    "          'data_root_dir'  : 'hft data',\n",
    "          'backup_interval': 1, # in min\n",
    "          'coins'          : coins,\n",
    "          'stable_coin'    : 'USDT',\n",
    "          'stream_freq'    : 100, # in ms\n",
    "          'num_of_entries' : 5\n",
    "         }\n",
    "\n",
    "dataset_fetcher = HFT_Dataset_Fetcher(params)\n",
    "dataset_fetcher.start_saving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "999d3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fetcher.__del__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31319ff3",
   "metadata": {},
   "source": [
    "##### TODO:                         \n",
    "\n",
    "        - When HFT data is needed:\n",
    "        - Move over to AWS\n",
    "            - EC2 2 GiB instance\n",
    "            - Bucket\n",
    "        - Start fetching on AWS\n",
    "        \n",
    "        \n",
    "        - AWS sources:\n",
    "                - https://www.bing.com/videos/search?q=aws+sagemaker+studio&&view=detail&mid=938501A6D4EE09EEFACB938501A6D4EE09EEFACB&&FORM=VRDGAR&ru=%2Fvideos%2Fsearch%3Fq%3Daws%2Bsagemaker%2Bstudio%26FORM%3DHDRSC4\n",
    "                \n",
    "                - Setup EC2 instances for Jupyter inside AWS\n",
    "                - Each instance has its own notebooks\n",
    "                -https://www.bing.com/videos/search?&q=aws+sagemaker+studio&view=detail&mid=C6A3CC0856C44720117EC6A3CC0856C44720117E&FORM=VDRVRV&ru=%2Fvideos%2Fsearch%3Fq%3Daws%2Bsagemaker%2Bstudio%26FORM%3DHDRSC4&ajaxhist=0\n",
    "            - test minimal freq: 50 ms\n",
    "            \n",
    "            \n",
    "            \n",
    "           -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d18a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
