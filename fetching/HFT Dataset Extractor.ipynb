{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174d4051",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "    - Use after having extracted hft data from HFT_Dataset_Fetcher inside dir\n",
    "    - Old and recent data can be used an even while HFT_Dataset_Fetcher is active\n",
    "    - Access a lot of raw data --> convert it to any wished dataset\n",
    "        • Clip datasets\n",
    "        • Downsample to multiples of 50 ms\n",
    "        • Avoid double saves\n",
    "        \n",
    "    - May run on higher AWS servers to get 8, 16 or 32 GiB of RAM for very big files\n",
    "        - Cheap (< 1€): takes less than an hour \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca0b33",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.error import  HTTPError\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import tqdm\n",
    "\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82ea55e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c8289a3f9eda>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-c8289a3f9eda>\"\u001b[1;36m, line \u001b[1;32m29\u001b[0m\n\u001b[1;33m    assert self.file_extension in ['pkl', 'csv'],\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Dataset_Extractor:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 hft_dataset_extractor_params):\n",
    "        \n",
    "        self.saving_extension   = dataset_extractor_params['saving_extension' ] # 'pkl' or 'csv'\n",
    "        self.backup_save_path   = dataset_extractor_params['backup_save_path' ] # 'hft data/backup saves/since_...'\n",
    "        self.extraction_path    = dataset_extractor_params['extraction_path'  ] # 'hft data/extracted datasets/[dir_name]'\n",
    "        self.fetch_data_dir     = dataset_extractor_params['fetch_data_dir'   ] # like 'since_2023-06-01 08:30'\n",
    "        self.start_date         = dataset_extractor_params['start_date'       ] # like '2015-05-01 09:00'; in UTC!\n",
    "        self.end_date           = dataset_extractor_params['end_date'         ] # like '2015-05-01 12:00'; in UTC!\n",
    "        self.coins              = dataset_extractor_params['coins'            ] # like ['BTC', 'ETH', ...]\n",
    "        self.stable_coin        = dataset_extractor_params['stable_coin'      ] # 'USTD'\n",
    "        self.downsaple_freq     = dataset_extractor_params['downsaple_freq'   ] # like 0.25 (in s)\n",
    "\n",
    "        self.start_date         = self.start_date[:16] # clip to minutes'2023-06-01 08:31'\n",
    "        self.end_date           = self.end_date[:16]\n",
    "        self.pair_list          = [c + self.stable_coin for c in self.coins]\n",
    "        self.thread             = threading.Thread(target = self.__observing_loop, args = ())\n",
    "        self.thread_is_running  = False\n",
    "        \n",
    "        self.__init_dirs()\n",
    "        \n",
    "        \n",
    "        # backup_dir_name <-- 'ALGO_USDT_100ms_2023-06-01 08:31'\n",
    "\n",
    "        self.dataset_dir        = self.data_root_dir + '/' + 'extracted datasets'\n",
    "        \n",
    "        self._init_dirs()\n",
    "        \n",
    "        assert self.file_extension in ['pkl', 'csv']\n",
    "        assert os.path.exists(self.extraction_path) == True\n",
    "        \n",
    "    def _init_dirs(self):\n",
    "        assert (os.path.exists(self.data_root_dir) == True), f'{self.data_root_dir} has to exist for extracting'\n",
    "        \n",
    "        # find 'since_2023-06-01 08:30' folder inside self.data_root_dir\n",
    "        # check if it is only one file\n",
    "        self.fetch_data_path        = self.save_dir + '/' + self.fetch_data_dir\n",
    "        \n",
    "        #\n",
    "\n",
    "\n",
    "    \n",
    "    self.dataset_dir        = self.data_root_dir + '/' + 'extracted datasets'\n",
    "\n",
    "    def combine_backup_saves(self): # save to full datasets\n",
    "        \"\"\" Function for combining backup saves of Dataset_Extractor \"\"\"\n",
    "        for coin in  tqdm.tqdm(self.coins, desc='Creating datasets for all coins'):\n",
    "            while not read_all_data:\n",
    "                #read\n",
    "            \n",
    "                #concat\n",
    "                \n",
    "                \n",
    "                \n",
    "                read_all_data = (self.start_date in self.df.index) and  self.end_date in self.df.index)\n",
    "                self.end_date\n",
    "            # set to time idx --> make na\n",
    "            # set_\n",
    "            # clipping\n",
    "            df = df[self.start_date : self.end_date][:-1]\n",
    "            # save\n",
    "            \n",
    "        \n",
    "        self.df.set_index('Request Time', inplace=True)\n",
    "        \n",
    "        \n",
    "        cols = ['Request Time', 'Response Delay'\n",
    "        'Bid_1', 'BidV_1', 'Bid_2', 'BidV_2', 'Bid_3', 'BidV_3', 'Bid_4', 'BidV_4', 'Bid_5', 'BidV_5',\n",
    "        'Ask_1', 'AskV_1', 'Ask_2', 'AskV_2', 'Ask_3', 'AskV_3', 'Ask_4', 'AskV_4', 'Ask_5', 'AskV_5']\n",
    "\n",
    "            fetch_df = pd.DataFrame([prepared_fetch_arr], columns=cols, index=np.arange(1))\n",
    "        self.df = pd.concat((self.df, fetch_df), axis=0)\n",
    "            \n",
    "            \n",
    "        # df, nan_inp, time_idx, concat, pkl or csv\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8347507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
